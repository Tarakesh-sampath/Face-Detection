{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Work\\Intern\\Gemicats\\Face-Detection\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import numpy as np\n",
    "import cv2\n",
    "from mtcnn import MTCNN\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlib_detector(img):\n",
    "    dlib_face_detector = dlib.get_frontal_face_detector()\n",
    "    dlib_face_locations = dlib_face_detector(img)\n",
    "    return dlib_face_locations  # xmin,ymin,xmax,ymax\n",
    "\n",
    "def mtcnn_detector_dlib(img):\n",
    "    face_detector = MTCNN()\n",
    "    face_info = face_detector.detect_faces(img)\n",
    "    dlib_format = []\n",
    "    for i in face_info:\n",
    "        x, y, width, height = i[\"box\"]\n",
    "        dlib_format.append(dlib.rectangle(x,y,x+width,y+height))\n",
    "    return dlib_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodings(img,face_locations,pose_predictor,face_encoder):\n",
    "    predictors = [pose_predictor(img, face_location) for face_location in face_locations]\n",
    "    return np.array(face_encoder.compute_face_descriptor(img, predictors[0], 1))\n",
    "\n",
    "\n",
    "pose_predictor = dlib.shape_predictor('Dlib_model/shape_predictor_68_face_landmarks.dat')\n",
    "face_encoder = dlib.face_recognition_model_v1('Dlib_model/dlib_face_recognition_resnet_model_v1.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "img = cv2.imread(\"images/single_face.jpg\")\n",
    "\n",
    "face_locations = dlib_detector(img)\n",
    "face_encodings = encodings(img,face_locations,pose_predictor,face_encoder)\n",
    "print(face_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully organized!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Paths\n",
    "input_dir = './Dataset_raw/original images/'\n",
    "output_dir = './Dataset_organised/'\n",
    "\n",
    "# Define split ratios (can be changed as needed)\n",
    "train_split = 0.8\n",
    "val_split = 0.1\n",
    "test_split = 0.1\n",
    "\n",
    "# Ensure the output directories exist\n",
    "splits = ['train', 'val', 'test']\n",
    "for split in splits:\n",
    "    for class_name in os.listdir(input_dir):\n",
    "        split_dir = os.path.join(output_dir, split, class_name)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "# Process each folder (person1, person2, etc.)\n",
    "for class_name in os.listdir(input_dir):\n",
    "    class_path = os.path.join(input_dir, class_name)\n",
    "    \n",
    "    if not os.path.isdir(class_path):\n",
    "        continue  # Skip files, only process directories (like person1, person2)\n",
    "    \n",
    "    # Get all image files in the current class folder\n",
    "    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "    random.shuffle(images)  # Shuffle to ensure randomness\n",
    "\n",
    "    # Split images into train, val, and test\n",
    "    total_images = len(images)\n",
    "    train_count = int(total_images * train_split)\n",
    "    val_count = int(total_images * val_split)\n",
    "    \n",
    "    train_images = images[:train_count]\n",
    "    val_images = images[train_count:train_count + val_count]\n",
    "    test_images = images[train_count + val_count:]\n",
    "\n",
    "    # Copy files to train, val, and test directories\n",
    "    for image_set, split in zip([train_images, val_images, test_images], splits):\n",
    "        for image_name in image_set:\n",
    "            src_path = os.path.join(class_path, image_name)\n",
    "            dst_path = os.path.join(output_dir, split, class_name, image_name)\n",
    "            \n",
    "            try:\n",
    "                shutil.copy(src_path, dst_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying {src_path} to {dst_path}: {e}\")\n",
    "\n",
    "print(\"Dataset successfully organized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths to models and directories\n",
    "pose_predictor = dlib.shape_predictor('Dlib_model/shape_predictor_68_face_landmarks.dat')\n",
    "face_encoder = dlib.face_recognition_model_v1('Dlib_model/dlib_face_recognition_resnet_model_v1.dat')\n",
    "input_dir = './Dataset_raw/original images/'\n",
    "output_dir = './Dataset_organised/'\n",
    "\n",
    "# Define split ratios (train, validation, test)\n",
    "train_split = 0.8\n",
    "val_split = 0.1\n",
    "test_split = 0.1\n",
    "\n",
    "# Ensure the output directories exist\n",
    "splits = ['train', 'val', 'test']\n",
    "for split in splits:\n",
    "    for class_name in os.listdir(input_dir):\n",
    "        split_dir = os.path.join(output_dir, split, class_name)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "# Function to extract embeddings\n",
    "def extract_embeddings(img_path):\n",
    "    img = Image.open(img_path)\n",
    "    img = np.array(img)\n",
    "    \n",
    "    face_locations = dlib_detector(img)\n",
    "    if not face_locations:\n",
    "        return None  # No faces detected\n",
    "    \n",
    "    # Extract embeddings for detected faces\n",
    "    embeddings = encodings(img, face_locations, pose_predictor, face_encoder)\n",
    "    return embeddings\n",
    "\n",
    "# Process each folder (person1, person2, etc.)\n",
    "dataset_dict = {'train': [], 'val': [], 'test': []}\n",
    "for class_name in os.listdir(input_dir):\n",
    "    class_path = os.path.join(input_dir, class_name)\n",
    "    \n",
    "    if not os.path.isdir(class_path):\n",
    "        continue  # Skip files, only process directories (like person1, person2)\n",
    "    \n",
    "    # Get all image files in the current class folder\n",
    "    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "    random.shuffle(images)  # Shuffle to ensure randomness\n",
    "\n",
    "    # Split images into train, val, and test\n",
    "    total_images = len(images)\n",
    "    train_count = int(total_images * train_split)\n",
    "    val_count = int(total_images * val_split)\n",
    "    \n",
    "    train_images = images[:train_count]\n",
    "    val_images = images[train_count:train_count + val_count]\n",
    "    test_images = images[train_count + val_count:]\n",
    "\n",
    "    # Add images to train, val, and test splits with embeddings\n",
    "    for split, image_set in zip(splits, [train_images, val_images, test_images]):\n",
    "        for image_name in image_set:\n",
    "            img_path = os.path.join(class_path, image_name)\n",
    "            # getting the embeding for the photo\n",
    "            # ide\n",
    "            embedding = extract_embeddings(img_path)\n",
    "            if embedding is not None:\n",
    "                dataset_dict[split].append({\n",
    "                    'image': img_path,\n",
    "                    'label': class_name,\n",
    "                    'embedding': embedding\n",
    "                })\n",
    "\n",
    "# Convert the dataset dictionary to Dataset objects\n",
    "datasets = {\n",
    "    split: Dataset.from_dict({\n",
    "        'image': [item['image'] for item in items],\n",
    "        'label': [item['label'] for item in items],\n",
    "        'embedding': [item['embedding'] for item in items]\n",
    "    })\n",
    "    for split, items in dataset_dict.items()\n",
    "}\n",
    "\n",
    "# Create the final DatasetDict\n",
    "dataset = DatasetDict(datasets)\n",
    "\n",
    "# Print out the structure of the dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(\"./Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28852154c4048d6ad9ec4435c83f1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853725e00bb24a82a3920eaacb9e9b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca0acb4ff6749bfb0f619f9e7c0d31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b167d2df488f417fb84bfdca989803c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/2037 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e53d82115c64a71a0d446b11956fe76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/244 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9465c77e7da4befa909ffb25d000526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/281 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a6f6240f64413c80478f07a066d311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d15b56495f34e779c1180bfa8e4dcb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65f3ca245504131bf888bd5cb230c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the local dataset\n",
    "dataset = load_dataset('imagefolder', data_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 2037\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 244\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 281\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function compute_embeddings at 0x000001BFA713FA30> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d7115e115d44099ac1117dd0dfcec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define embedding extraction function\n",
    "def compute_embeddings(example):\n",
    "    img = example[\"image\"]  # Load the image\n",
    "    img = np.array(img)  # Ensure the image is in numpy array format\n",
    "    \n",
    "    # Detect face locations\n",
    "    face_locations = dlib_detector(img)\n",
    "    \n",
    "    # If no face is detected, return a zero array\n",
    "    if len(face_locations) == 0:\n",
    "        example[\"embeddings\"] = np.zeros(128)  # Assuming embeddings are 128-dimensional\n",
    "        return example\n",
    "    \n",
    "    # Compute embeddings\n",
    "    face_encodings = encodings(img, face_locations, pose_predictor, face_encoder)\n",
    "    \n",
    "    # Store embeddings in the example\n",
    "    example[\"embeddings\"] = face_encodings.tolist()  # Convert to list for serialization\n",
    "    return example\n",
    "\n",
    "# Apply the function to the DatasetDict\n",
    "# This assumes dataset['train'], dataset['validation'], etc., are present\n",
    "dataset_with_embeddings = dataset.map(compute_embeddings, batched=False)\n",
    "\n",
    "# Inspect the updated dataset\n",
    "print(dataset_with_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
