{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import DatasetDict, Dataset ,load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Work\\Intern\\Gemicats\\Face-Detection\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Work\\Intern\\Gemicats\\Face-Detection\\venv\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Work\\Intern\\Gemicats\\Face-Detection\\venv\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Processing images and extracting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Akshay Kumar: 100%|██████████| 50/50 [00:09<00:00,  5.02it/s]\n",
      "Processing Alexandra Daddario: 100%|██████████| 92/92 [00:14<00:00,  6.23it/s]\n",
      "Processing Alia Bhatt: 100%|██████████| 79/79 [00:12<00:00,  6.12it/s]\n",
      "Processing Amitabh Bachchan: 100%|██████████| 74/74 [00:11<00:00,  6.52it/s]\n",
      "Processing Andy Samberg: 100%|██████████| 92/92 [00:14<00:00,  6.16it/s]\n",
      "Processing Anushka Sharma: 100%|██████████| 68/68 [00:10<00:00,  6.41it/s]\n",
      "Processing Billie Eilish: 100%|██████████| 98/98 [00:16<00:00,  6.00it/s]\n",
      "Processing Brad Pitt: 100%|██████████| 120/120 [00:19<00:00,  6.09it/s]\n",
      "Processing Camila Cabello: 100%|██████████| 87/87 [00:14<00:00,  6.12it/s]\n",
      "Processing Charlize Theron: 100%|██████████| 78/78 [00:12<00:00,  6.21it/s]\n",
      "Processing Claire Holt: 100%|██████████| 96/96 [00:15<00:00,  6.21it/s]\n",
      "Processing Courtney Cox: 100%|██████████| 80/80 [00:12<00:00,  6.24it/s]\n",
      "Processing Dwayne Johnson: 100%|██████████| 61/61 [00:09<00:00,  6.49it/s]\n",
      "Processing Elizabeth Olsen: 100%|██████████| 71/71 [00:11<00:00,  5.92it/s]\n",
      "Processing Ellen Degeneres: 100%|██████████| 75/75 [00:12<00:00,  6.12it/s]\n",
      "Processing Henry Cavill: 100%|██████████| 106/106 [00:18<00:00,  5.73it/s]\n",
      "Processing Hrithik Roshan: 100%|██████████| 101/101 [00:16<00:00,  6.01it/s]\n",
      "Processing Hugh Jackman: 100%|██████████| 112/112 [00:19<00:00,  5.85it/s]\n",
      "Processing Jessica Alba: 100%|██████████| 108/108 [00:19<00:00,  5.57it/s]\n",
      "Processing Kashyap: 100%|██████████| 30/30 [00:07<00:00,  3.97it/s]\n",
      "Processing Lisa Kudrow: 100%|██████████| 70/70 [00:11<00:00,  6.18it/s]\n",
      "Processing Margot Robbie: 100%|██████████| 72/72 [00:12<00:00,  5.89it/s]\n",
      "Processing Marmik: 100%|██████████| 32/32 [00:06<00:00,  4.58it/s]\n",
      "Processing Natalie Portman: 100%|██████████| 105/105 [00:17<00:00,  6.03it/s]\n",
      "Processing Priyanka Chopra: 100%|██████████| 102/102 [00:16<00:00,  6.15it/s]\n",
      "Processing Robert Downey Jr: 100%|██████████| 113/113 [00:17<00:00,  6.40it/s]\n",
      "Processing Roger Federer: 100%|██████████| 77/77 [00:12<00:00,  6.02it/s]\n",
      "Processing Tom Cruise: 100%|██████████| 58/58 [00:09<00:00,  6.24it/s]\n",
      "Processing Vijay Deverakonda: 100%|██████████| 115/115 [00:17<00:00,  6.52it/s]\n",
      "Processing Virat Kohli: 100%|██████████| 49/49 [00:07<00:00,  6.14it/s]\n",
      "Processing Zac Efron: 100%|██████████| 91/91 [00:14<00:00,  6.21it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca56e1af97ec485ab440789fe3f9c266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2049 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d194631008441a942895c5a5bd7698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f544963bbae4582a2d690a98132f97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/257 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict saved to 'huggingface_dataset'.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image_path', 'embedding', 'label'],\n",
      "        num_rows: 2049\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image_path', 'embedding', 'label'],\n",
      "        num_rows: 256\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image_path', 'embedding', 'label'],\n",
      "        num_rows: 257\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set base directory for dataset\n",
    "base_dir = \"Dataset_new\"  # Change to your dataset folder path\n",
    "\n",
    "# 1. Load pre-trained model (EfficientNetB0)\n",
    "pretrained_model = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False,  # Remove the classification head\n",
    "    pooling=\"avg\",      # Use Global Average Pooling for embeddings\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# 2. Helper function to preprocess images\n",
    "def preprocess_image(image_path):\n",
    "    # Load and resize image\n",
    "    image = tf.keras.utils.load_img(image_path, target_size=(224, 224))\n",
    "    image_array = tf.keras.utils.img_to_array(image)\n",
    "    # Preprocess for EfficientNetB0\n",
    "    return tf.keras.applications.efficientnet.preprocess_input(image_array)\n",
    "\n",
    "# 3. Collect image paths, embeddings, and labels\n",
    "image_paths = []\n",
    "embeddings = []\n",
    "labels = []\n",
    "label_map = {}\n",
    "\n",
    "# Traverse directories and collect data\n",
    "print(\"Processing images and extracting embeddings...\")\n",
    "for label_idx, label_name in enumerate(os.listdir(base_dir)):\n",
    "    label_dir = os.path.join(base_dir, label_name)\n",
    "    if not os.path.isdir(label_dir):\n",
    "        continue\n",
    "\n",
    "    # Map label to an integer\n",
    "    label_map[label_name] = label_idx\n",
    "\n",
    "    for img_name in tqdm(os.listdir(label_dir), desc=f\"Processing {label_name}\"):\n",
    "        img_path = os.path.join(label_dir, img_name)\n",
    "        try:\n",
    "            # Preprocess image\n",
    "            image = preprocess_image(img_path)\n",
    "            image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "\n",
    "            # Get embedding\n",
    "            embedding = pretrained_model.predict(image, verbose=0).squeeze()\n",
    "\n",
    "            # Append data\n",
    "            image_paths.append(img_path)\n",
    "            embeddings.append(embedding)\n",
    "            labels.append(label_idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "# 4. Create a dictionary of data\n",
    "data_dict = {\n",
    "    \"image_path\": image_paths,\n",
    "    \"embedding\": embeddings,\n",
    "    \"label\": labels\n",
    "}\n",
    "\n",
    "# 5. Train-Test-Validation Split\n",
    "train_idx, test_idx = train_test_split(range(len(labels)), test_size=0.2, stratify=labels, random_state=42)\n",
    "test_idx, val_idx = train_test_split(test_idx, test_size=0.5, stratify=[labels[i] for i in test_idx], random_state=42)\n",
    "\n",
    "# Helper function to create a Dataset\n",
    "def create_split(indices):\n",
    "    return Dataset.from_dict({\n",
    "        \"image_path\": [data_dict[\"image_path\"][i] for i in indices],\n",
    "        \"embedding\": [data_dict[\"embedding\"][i] for i in indices],\n",
    "        \"label\": [data_dict[\"label\"][i] for i in indices],\n",
    "    })\n",
    "\n",
    "# Create DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": create_split(train_idx),\n",
    "    \"test\": create_split(test_idx),\n",
    "    \"validation\": create_split(val_idx)\n",
    "})\n",
    "\n",
    "# 6. Save DatasetDict\n",
    "dataset_dict.save_to_disk(\"huggingface_dataset\")\n",
    "print(\"DatasetDict saved to 'huggingface_dataset'.\")\n",
    "\n",
    "# Optional: Print dataset info\n",
    "print(dataset_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"huggingface_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6dc03db99944bfbc0caeb511aa24d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a483b9dc363404586df6e932a3d2a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3537e8cae9bb43adacbc3c1463f06faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143133f088ea4ba0a6883be6e7dab3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e940156ee6e41f9a54d4a7cd9da0a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d6d7f4ff584eb19e1e0347ceb5e58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Tarakeshwaran/sampleface30-Dataset/commit/bef9c22e5a37b991355e1401180538c1f01e4dbf', commit_message='Upload dataset', commit_description='', oid='bef9c22e5a37b991355e1401180538c1f01e4dbf', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Tarakeshwaran/sampleface30-Dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Tarakeshwaran/sampleface30-Dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"Tarakeshwaran/sampleface30-Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
