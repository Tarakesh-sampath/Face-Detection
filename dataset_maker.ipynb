{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Work\\Intern\\Gemicats\\Face-Detection\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import DatasetDict, Dataset ,load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Work\\Intern\\Gemicats\\Face-Detection\\venv\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Work\\Intern\\Gemicats\\Face-Detection\\venv\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Processing images and extracting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Akshay Kumar: 100%|██████████| 50/50 [00:06<00:00,  7.67it/s]\n",
      "Processing Alexandra Daddario: 100%|██████████| 92/92 [00:09<00:00,  9.87it/s]\n",
      "Processing Alia Bhatt: 100%|██████████| 79/79 [00:08<00:00,  9.69it/s]\n",
      "Processing Amitabh Bachchan: 100%|██████████| 74/74 [00:07<00:00, 10.13it/s]\n",
      "Processing Andy Samberg: 100%|██████████| 92/92 [00:09<00:00,  9.97it/s]\n",
      "Processing Anushka Sharma: 100%|██████████| 68/68 [00:06<00:00,  9.80it/s]\n",
      "Processing Billie Eilish: 100%|██████████| 98/98 [00:10<00:00,  9.43it/s]\n",
      "Processing Brad Pitt: 100%|██████████| 120/120 [00:12<00:00,  9.71it/s]\n",
      "Processing Camila Cabello: 100%|██████████| 87/87 [00:09<00:00,  9.49it/s]\n",
      "Processing Charlize Theron: 100%|██████████| 78/78 [00:08<00:00,  9.48it/s]\n",
      "Processing Claire Holt: 100%|██████████| 96/96 [00:09<00:00,  9.62it/s]\n",
      "Processing Courtney Cox: 100%|██████████| 80/80 [00:08<00:00,  9.90it/s]\n",
      "Processing Dwayne Johnson: 100%|██████████| 61/61 [00:06<00:00,  9.98it/s]\n",
      "Processing Elizabeth Olsen: 100%|██████████| 71/71 [00:07<00:00,  9.56it/s]\n",
      "Processing Ellen Degeneres: 100%|██████████| 75/75 [00:08<00:00,  9.29it/s]\n",
      "Processing Henry Cavill: 100%|██████████| 106/106 [00:11<00:00,  9.29it/s]\n",
      "Processing Hrithik Roshan: 100%|██████████| 101/101 [00:11<00:00,  8.80it/s]\n",
      "Processing Hugh Jackman: 100%|██████████| 112/112 [00:11<00:00,  9.37it/s]\n",
      "Processing Jessica Alba: 100%|██████████| 108/108 [00:13<00:00,  8.28it/s]\n",
      "Processing Kashyap: 100%|██████████| 30/30 [00:04<00:00,  6.53it/s]\n",
      "Processing Lisa Kudrow: 100%|██████████| 70/70 [00:08<00:00,  7.80it/s]\n",
      "Processing Margot Robbie: 100%|██████████| 72/72 [00:08<00:00,  8.51it/s]\n",
      "Processing Marmik: 100%|██████████| 32/32 [00:04<00:00,  6.74it/s]\n",
      "Processing Natalie Portman: 100%|██████████| 105/105 [00:12<00:00,  8.61it/s]\n",
      "Processing Priyanka Chopra: 100%|██████████| 102/102 [00:11<00:00,  8.61it/s]\n",
      "Processing Robert Downey Jr: 100%|██████████| 113/113 [00:12<00:00,  9.08it/s]\n",
      "Processing Roger Federer: 100%|██████████| 77/77 [00:08<00:00,  9.41it/s]\n",
      "Processing Tom Cruise: 100%|██████████| 58/58 [00:06<00:00,  9.35it/s]\n",
      "Processing Vijay Deverakonda: 100%|██████████| 115/115 [00:11<00:00,  9.83it/s]\n",
      "Processing Virat Kohli: 100%|██████████| 49/49 [00:05<00:00,  9.59it/s]\n",
      "Processing Zac Efron: 100%|██████████| 91/91 [00:09<00:00,  9.53it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8ebb1d787944f2aea861841555d558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0186ec975a134c98b9da18a374f886da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6639a42a6ad54780ba2927967c19602a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba178808d94b4ce1a156b0431686f803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image_path', 'embedding', 'label'],\n",
      "        num_rows: 1537\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image_path', 'embedding', 'label'],\n",
      "        num_rows: 1025\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Set base directory for dataset\n",
    "base_dir = \"Dataset_raw\"  # Change to your dataset folder path\n",
    "\n",
    "# 1. Load pre-trained model (EfficientNetB0)\n",
    "pretrained_model = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False,  # Remove the classification head\n",
    "    pooling=\"avg\",      # Use Global Average Pooling for embeddings\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# 2. Helper function to preprocess images\n",
    "def preprocess_image(image_path):\n",
    "    # Load and resize image\n",
    "    image = tf.keras.utils.load_img(image_path, target_size=(224, 224))\n",
    "    image_array = tf.keras.utils.img_to_array(image)\n",
    "    # Preprocess for EfficientNetB0\n",
    "    return tf.keras.applications.efficientnet.preprocess_input(image_array)\n",
    "\n",
    "# 3. Collect image paths, embeddings, and labels\n",
    "image_paths = []\n",
    "embeddings = []\n",
    "labels = []\n",
    "label_map = {}\n",
    "\n",
    "# Traverse directories and collect data\n",
    "print(\"Processing images and extracting embeddings...\")\n",
    "for label_idx, label_name in enumerate(os.listdir(base_dir)):\n",
    "    label_dir = os.path.join(base_dir, label_name)\n",
    "    if not os.path.isdir(label_dir):\n",
    "        continue\n",
    "\n",
    "    # Map label to an integer\n",
    "    label_map[label_name] = label_idx\n",
    "\n",
    "    for img_name in tqdm(os.listdir(label_dir), desc=f\"Processing {label_name}\"):\n",
    "        img_path = os.path.join(label_dir, img_name)\n",
    "        try:\n",
    "            # Preprocess image\n",
    "            image = preprocess_image(img_path)\n",
    "            image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "\n",
    "            # Get embedding\n",
    "            embedding = pretrained_model.predict(image, verbose=0).squeeze()\n",
    "\n",
    "            # Append data\n",
    "            image_paths.append(img_path)\n",
    "            embeddings.append(embedding)\n",
    "            labels.append(label_idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "# 4. Create a dictionary of data\n",
    "data_dict = {\n",
    "    \"image_path\": image_paths,\n",
    "    \"embedding\": embeddings,\n",
    "    \"label\": labels\n",
    "}\n",
    "\n",
    "# 5. Train-Test-Validation Split\n",
    "train_idx, test_idx = train_test_split(range(len(labels)), test_size=0.3, stratify=labels, random_state=42)\n",
    "#test_idx, val_idx = train_test_split(test_idx, test_size=0.25, stratify=[labels[i] for i in test_idx], random_state=42)\n",
    "\n",
    "# Helper function to create a Dataset\n",
    "def create_split(indices):\n",
    "    return Dataset.from_dict({\n",
    "        \"image_path\": [data_dict[\"image_path\"][i] for i in indices],\n",
    "        \"embedding\": [data_dict[\"embedding\"][i] for i in indices],\n",
    "        \"label\": [data_dict[\"label\"][i] for i in indices],\n",
    "    })\n",
    "\n",
    "# Create DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": create_split(train_idx),\n",
    "    \"test\": create_split(test_idx),\n",
    "    #\"validation\": create_split(val_idx)\n",
    "})\n",
    "\n",
    "# 6. Save DatasetDict\n",
    "dataset_dict.push_to_hub(\"Tarakeshwaran/sampleface30-Dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Print dataset info\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images and extracting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Akshay Kumar: 100%|██████████| 50/50 [00:13<00:00,  3.82it/s]\n",
      "Processing Alexandra Daddario: 100%|██████████| 92/92 [00:24<00:00,  3.81it/s]\n",
      "Processing Alia Bhatt: 100%|██████████| 79/79 [00:21<00:00,  3.68it/s]\n",
      "Processing Amitabh Bachchan: 100%|██████████| 74/74 [00:17<00:00,  4.15it/s]\n",
      "Processing Andy Samberg: 100%|██████████| 92/92 [00:22<00:00,  4.18it/s]\n",
      "Processing Anushka Sharma: 100%|██████████| 68/68 [00:15<00:00,  4.34it/s]\n",
      "Processing Billie Eilish: 100%|██████████| 98/98 [00:23<00:00,  4.16it/s]\n",
      "Processing Brad Pitt: 100%|██████████| 120/120 [00:29<00:00,  4.10it/s]\n",
      "Processing Camila Cabello: 100%|██████████| 87/87 [00:20<00:00,  4.26it/s]\n",
      "Processing Charlize Theron: 100%|██████████| 78/78 [00:19<00:00,  4.10it/s]\n",
      "Processing Claire Holt: 100%|██████████| 96/96 [00:22<00:00,  4.19it/s]\n",
      "Processing Courtney Cox: 100%|██████████| 80/80 [00:19<00:00,  4.18it/s]\n",
      "Processing Dwayne Johnson: 100%|██████████| 61/61 [00:14<00:00,  4.34it/s]\n",
      "Processing Elizabeth Olsen: 100%|██████████| 71/71 [00:16<00:00,  4.29it/s]\n",
      "Processing Ellen Degeneres: 100%|██████████| 75/75 [00:17<00:00,  4.32it/s]\n",
      "Processing Henry Cavill: 100%|██████████| 106/106 [00:25<00:00,  4.17it/s]\n",
      "Processing Hrithik Roshan: 100%|██████████| 101/101 [00:22<00:00,  4.50it/s]\n",
      "Processing Hugh Jackman: 100%|██████████| 112/112 [00:25<00:00,  4.45it/s]\n",
      "Processing Jessica Alba: 100%|██████████| 108/108 [00:24<00:00,  4.36it/s]\n",
      "Processing Kashyap: 100%|██████████| 30/30 [00:07<00:00,  3.77it/s]\n",
      "Processing Lisa Kudrow: 100%|██████████| 70/70 [00:16<00:00,  4.37it/s]\n",
      "Processing Margot Robbie: 100%|██████████| 72/72 [00:16<00:00,  4.32it/s]\n",
      "Processing Marmik: 100%|██████████| 32/32 [00:08<00:00,  3.73it/s]\n",
      "Processing Natalie Portman: 100%|██████████| 105/105 [00:27<00:00,  3.89it/s]\n",
      "Processing Priyanka Chopra: 100%|██████████| 102/102 [00:23<00:00,  4.25it/s]\n",
      "Processing Robert Downey Jr: 100%|██████████| 113/113 [00:25<00:00,  4.35it/s]\n",
      "Processing Roger Federer: 100%|██████████| 77/77 [00:17<00:00,  4.33it/s]\n",
      "Processing Tom Cruise: 100%|██████████| 58/58 [00:13<00:00,  4.34it/s]\n",
      "Processing Vijay Deverakonda: 100%|██████████| 115/115 [00:26<00:00,  4.36it/s]\n",
      "Processing Virat Kohli: 100%|██████████| 49/49 [00:12<00:00,  4.04it/s]\n",
      "Processing Zac Efron: 100%|██████████| 91/91 [00:21<00:00,  4.33it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80484fdad4794595993da27bb93be5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e0c4828cc142d4a70569c656d21f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfbd7b0953db4699bd492e389e9a4769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8b37440e1545e8b2a86c2dc2bbb8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Tarakeshwaran/sampleface30-Dataset/commit/ae4a85d6ac8a4a07c09b7c6c3772e0a32eb0a17b', commit_message='Upload dataset', commit_description='', oid='ae4a85d6ac8a4a07c09b7c6c3772e0a32eb0a17b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Tarakeshwaran/sampleface30-Dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Tarakeshwaran/sampleface30-Dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set base directory for dataset\n",
    "base_dir = \"Dataset_raw\"  # Change to your dataset folder path\n",
    "\n",
    "# 1. Load pre-trained model (EfficientNetB0)\n",
    "pretrained_model = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False,  # Remove the classification head\n",
    "    pooling=\"avg\",      # Use Global Average Pooling for embeddings\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# 2. Data augmentation generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    brightness_range=(0.8, 1.2),\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input\n",
    ")\n",
    "\n",
    "# 3. Helper function to preprocess images\n",
    "def preprocess_image(image_path):\n",
    "    # Load and resize image\n",
    "    image = tf.keras.utils.load_img(image_path, target_size=(224, 224))\n",
    "    image_array = tf.keras.utils.img_to_array(image)\n",
    "    # Preprocess for EfficientNetB0\n",
    "    return tf.keras.applications.efficientnet.preprocess_input(image_array)\n",
    "\n",
    "# 4. Collect embeddings and labels\n",
    "embeddings = []\n",
    "labels = []\n",
    "label_map = {}\n",
    "\n",
    "print(\"Processing images and extracting embeddings...\")\n",
    "for label_idx, label_name in enumerate(os.listdir(base_dir)):\n",
    "    label_dir = os.path.join(base_dir, label_name)\n",
    "    if not os.path.isdir(label_dir):\n",
    "        continue\n",
    "\n",
    "    # Map label to an integer\n",
    "    label_map[label_name] = label_idx\n",
    "\n",
    "    for img_name in tqdm(os.listdir(label_dir), desc=f\"Processing {label_name}\"):\n",
    "        img_path = os.path.join(label_dir, img_name)\n",
    "        try:\n",
    "            # Preprocess image\n",
    "            image = preprocess_image(img_path)\n",
    "\n",
    "            # Apply augmentation to the image\n",
    "            image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "            augmented_images = datagen.flow(image, batch_size=1)\n",
    "\n",
    "            # Process the original image and a batch of augmented images\n",
    "            for _ in range(2):  # Generate 2 augmented samples per original\n",
    "                augmented_image = next(augmented_images)[0]\n",
    "\n",
    "                # Get embedding\n",
    "                embedding = pretrained_model.predict(\n",
    "                    np.expand_dims(augmented_image, axis=0), verbose=0\n",
    "                ).squeeze()\n",
    "\n",
    "                # Append data\n",
    "                embeddings.append(embedding)\n",
    "                labels.append(label_idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "# 5. Create a dictionary of data (exclude image paths)\n",
    "data_dict = {\n",
    "    \"embedding\": embeddings,\n",
    "    \"label\": labels\n",
    "}\n",
    "\n",
    "# 6. Helper function to create a Dataset\n",
    "def create_split(indices):\n",
    "    return Dataset.from_dict({\n",
    "        \"embedding\": [data_dict[\"embedding\"][i] for i in indices],\n",
    "        \"label\": [data_dict[\"label\"][i] for i in indices],\n",
    "    })\n",
    "\n",
    "# 7. Train-Test Split\n",
    "train_idx, test_idx = train_test_split(range(len(labels)), test_size=0.3, stratify=labels, random_state=42)\n",
    "\n",
    "# 8. Create DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": create_split(train_idx),\n",
    "    \"test\": create_split(test_idx),\n",
    "})\n",
    "\n",
    "\n",
    "# 9. Save DatasetDict\n",
    "dataset_dict.push_to_hub(\"Tarakeshwaran/sampleface30-Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
